{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Attention Visualization\n",
    "- By running this script, you can get images of attention-based-color-coded Set-MNIST.\n",
    "## To run this code...\n",
    "- You should prepare the summary file by running sample_and_summarize.py with a trained checkpoint.\n",
    "- You should install below libraries.\n",
    "    - matplotlib\n",
    "    - open3d\n",
    "    - numpy\n",
    "    - torch\n",
    "    - torchvision\n",
    "    - tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.no_grad()\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from draw import draw, draw_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set directories\n",
    "1. summary file path: summary_name\n",
    "2. path to save images: save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'images_attn'\n",
    "experiment_name = 'cd138mm_zscales222'\n",
    "summary_name = os.path.join('../checkpoints/gen/', experiment_name, 'summary.pth')\n",
    "\n",
    "imgdir = os.path.join(save_dir, experiment_name)\n",
    "imgdir_gt = os.path.join(imgdir, 'gt')\n",
    "imgdir_recon = os.path.join(imgdir, 'recon')\n",
    "imgdir_gen = os.path.join(imgdir, 'gen')\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(imgdir_gt, exist_ok=True)\n",
    "os.makedirs(imgdir_recon, exist_ok=True)\n",
    "os.makedirs(imgdir_gen, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smp_set: torch.Size([7, 2500, 20])\n",
      "smp_mask: torch.Size([7, 2500])\n",
      "smp_att: 3\n",
      "priors: 4\n",
      "recon_set: torch.Size([7, 2500, 20])\n",
      "recon_mask: torch.Size([7, 2500])\n",
      "posteriors: 4\n",
      "dec_att: 3\n",
      "enc_att: 3\n",
      "gt_set: torch.Size([7, 3414, 20])\n",
      "gt_mask: torch.Size([7, 3414])\n",
      "mean: torch.Size([1])\n",
      "std: torch.Size([1])\n",
      "sid: 1\n",
      "mid: 1\n",
      "cardinality: 1\n"
     ]
    }
   ],
   "source": [
    "summary = torch.load(summary_name)\n",
    "for k, v in summary.items():\n",
    "    try:\n",
    "        print(f\"{k}: {v.shape}\")\n",
    "    except AttributeError:\n",
    "        print(f\"{k}: {len(v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the samples to visualize\n",
    "- parse the samples by index.\n",
    "- below default code will visualize all samples. **Warning: Requires Huge Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_targets = list(range(len(summary['gt_mask'])))[:]\n",
    "gen_targets = list(range(len(summary['smp_mask'])))[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_att = len(summary['dec_att'])\n",
    "gt = summary['gt_set'][recon_targets]\n",
    "gt_mask = summary['gt_mask'][recon_targets]\n",
    "\n",
    "recon = summary['recon_set'][recon_targets]\n",
    "recon_mask = summary['recon_mask'][recon_targets]\n",
    "\n",
    "dec_att = [summary['dec_att'][l][:, :, recon_targets] for l in range(len_att)]\n",
    "enc_att = [summary['enc_att'][l][:, :, recon_targets] for l in range(len_att)]\n",
    "\n",
    "gen = summary['smp_set'][gen_targets]\n",
    "gen_mask = summary['smp_mask'][gen_targets]\n",
    "gen_att = [summary['smp_att'][l][:, :, gen_targets] for l in range(len_att)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Attention\n",
    "- lidx: index of layer\n",
    "- projection: ISAB has 2 projection attention and back-projection attention.\n",
    "    - 0: projection, 1: back-projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_selector(gt, gt_mask, att, lidx=0, projection=0):\n",
    "    return draw_attention(gt, gt_mask, att[lidx][projection], color_opt='gist_rainbow', dot_size=300)  # use 300 for multimnist, 700 for mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Encoder Attention on GT samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19350b55fd444928a9edd92d2c18c91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: received input with 20 dimensions; only visualizing the first 3 [this is expected for RNA-seq data]\n",
      "\u001b[1;33m[Open3D WARNING] GLFW Error: X11: The DISPLAY environment variable is missing\u001b[0;m\n",
      "\u001b[1;33m[Open3D WARNING] Failed to initialize GLFW\u001b[0;m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'point_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m topdown \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(enc_att))):\n\u001b[1;32m      2\u001b[0m     \u001b[39mfor\u001b[39;00m projection \u001b[39min\u001b[39;00m [\u001b[39m0\u001b[39m]:\n\u001b[0;32m----> 3\u001b[0m         gt_imgs \u001b[39m=\u001b[39m attention_selector(gt, gt_mask, enc_att, \u001b[39mlen\u001b[39;49m(enc_att) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m topdown, projection)\n\u001b[1;32m      4\u001b[0m         gt_imgs \u001b[39m=\u001b[39m [i\u001b[39m/\u001b[39m\u001b[39m255.\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m gt_imgs]\n\u001b[1;32m      5\u001b[0m         \u001b[39mfor\u001b[39;00m head \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(enc_att[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m, in \u001b[0;36mattention_selector\u001b[0;34m(gt, gt_mask, att, lidx, projection)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mattention_selector\u001b[39m(gt, gt_mask, att, lidx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, projection\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mreturn\u001b[39;00m draw_attention(gt, gt_mask, att[lidx][projection], color_opt\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgist_rainbow\u001b[39;49m\u001b[39m'\u001b[39;49m, dot_size\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/conda/rpeyser/envs/scset3/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/GitHub/scSet/figures/draw.py:500\u001b[0m, in \u001b[0;36mdraw_attention\u001b[0;34m(x, x_mask, alpha, back_color, W, color_opt, dot_size, palette_permutation)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWARNING: received input with 20 dimensions; only visualizing the first 3 [this is expected for RNA-seq data]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    499\u001b[0m     x_trunc \u001b[39m=\u001b[39m x[:, :, :\u001b[39m3\u001b[39m]\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mreturn\u001b[39;00m draw_attention_open3d(x_trunc, x_mask, alpha, color_opt\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgist_rainbow\u001b[39;49m\u001b[39m'\u001b[39;49m, size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, palette_permutation\u001b[39m=\u001b[39;49mpalette_permutation)\n\u001b[1;32m    501\u001b[0m     \u001b[39m#return draw_attention_pointcloud(x_trunc, x_mask, alpha=torch.ones(4, x.shape[0], x.shape[1], 16)) #:param alpha: Tensor([n_heads, B, N, I])\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/rpeyser/envs/scset3/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/GitHub/scSet/figures/draw.py:407\u001b[0m, in \u001b[0;36mdraw_attention_open3d\u001b[0;34m(x, x_mask, alpha, config, color_opt, view, size, palette_permutation)\u001b[0m\n\u001b[1;32m    405\u001b[0m     o3d\u001b[39m.\u001b[39mvisualization\u001b[39m.\u001b[39mdraw_geometries([pcl])\n\u001b[1;32m    406\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     img \u001b[39m=\u001b[39m o3d2numpy(pcl, config)\n\u001b[1;32m    408\u001b[0m     \u001b[39m#                 plt.imshow(img)\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[39m#                 plt.show()\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     imgs\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mtensor(img))\n",
      "File \u001b[0;32m~/GitHub/scSet/figures/draw.py:348\u001b[0m, in \u001b[0;36mo3d2numpy\u001b[0;34m(geo, config)\u001b[0m\n\u001b[1;32m    345\u001b[0m vis\u001b[39m.\u001b[39mcreate_window(visible\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    347\u001b[0m opt \u001b[39m=\u001b[39m vis\u001b[39m.\u001b[39mget_render_option()\n\u001b[0;32m--> 348\u001b[0m opt\u001b[39m.\u001b[39;49mpoint_size \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m    349\u001b[0m opt\u001b[39m.\u001b[39mlight_on \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    351\u001b[0m vis\u001b[39m.\u001b[39madd_geometry(geo)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'point_size'"
     ]
    }
   ],
   "source": [
    "for topdown in tqdm(range(len(enc_att))):\n",
    "    for projection in [0]:\n",
    "        gt_imgs = attention_selector(gt, gt_mask, enc_att, len(enc_att) - 1 - topdown, projection)\n",
    "        gt_imgs = [i/255. for i in gt_imgs]\n",
    "        for head in range(enc_att[0][0].shape[0]):\n",
    "            for idx in range(len(recon_targets)):\n",
    "                data_idx = recon_targets[idx]\n",
    "                gt_img = gt_imgs[idx][head]\n",
    "                save_image(gt_img, os.path.join(imgdir_gt, f'{topdown}_{projection}_{head}_{data_idx}.png'))\n",
    "del gt_imgs\n",
    "print('gt DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Decoder Attention on Reconstructed samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee64e23f7b95497a8d53a8029c2b2c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: received input with 20 dimensions; only visualizing the first 3 [this is expected for RNA-seq data]\n",
      "WARNING: received input with 20 dimensions; only visualizing the first 3 [this is expected for RNA-seq data]\n",
      "WARNING: received input with 20 dimensions; only visualizing the first 3 [this is expected for RNA-seq data]\n",
      "\n",
      "recon DONE\n"
     ]
    }
   ],
   "source": [
    "for topdown in tqdm(range(len(enc_att))):\n",
    "    for projection in [1]:\n",
    "        recon_imgs = attention_selector(recon, recon_mask, dec_att, topdown, projection)\n",
    "        recon_imgs = [i/255. for i in recon_imgs]\n",
    "        for head in range(enc_att[0][0].shape[0]):\n",
    "            for idx in range(len(recon_targets)):\n",
    "                data_idx = recon_targets[idx]\n",
    "                recon_img = recon_imgs[idx][head]\n",
    "                save_image(recon_img, os.path.join(imgdir_recon, f'{topdown}_{projection}_{head}_{data_idx}.png'))\n",
    "del recon_imgs\n",
    "print('recon DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Decoder Attention on Generated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0974b0c79d41cfb97020f0f0384f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: received input with 20 dimensions; only visualizing the first 3 [this is expected for RNA-seq data]\n",
      "WARNING: received input with 20 dimensions; only visualizing the first 3 [this is expected for RNA-seq data]\n",
      "WARNING: received input with 20 dimensions; only visualizing the first 3 [this is expected for RNA-seq data]\n",
      "\n",
      "gen DONE\n"
     ]
    }
   ],
   "source": [
    "for topdown in tqdm(range(len(dec_att))):\n",
    "    for projection in [1,]:\n",
    "        gen_imgs = attention_selector(gen, gen_mask, gen_att, topdown, projection)\n",
    "        gen_imgs = [i/255. for i in gen_imgs]\n",
    "        for head in range(enc_att[0][0].shape[0]):\n",
    "            for idx in range(len(gen_targets)):\n",
    "                data_idx = gen_targets[idx]\n",
    "                gen_img = gen_imgs[idx][head]\n",
    "                save_image(gen_img.float(), os.path.join(imgdir_gen, f'{topdown}_{projection}_{head}_{data_idx}.png'))\n",
    "        del gen_imgs\n",
    "print('gen DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scset3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "30314d88031b0d7ffe696dc27ab20b7552f37a4dac8c459a5ad3e167b97b3a2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
